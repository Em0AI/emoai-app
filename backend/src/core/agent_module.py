# agent_module.py
import numpy as np
import re
from config import client # å¯¼å…¥ client
from utils import safe_text
from state import agent_scores, agent_feedback_memory, user_prefs # å¯¼å…¥ state

# 8. å­¦ä¹ å‹äººæ ¼ç³»ç»Ÿ
AGENTS = {
"empathetic": {
    "role": "kind listener",
    "objective": "reply with gentle understanding and short, natural sentences",
    "tone": "soft and conversational",
    "avoid": "therapy clichÃ©s or exaggerated sympathy"
},

"counselor": {
    "role": "empathetic listener",
    "objective": " without judging or fixing",
    "tone": "warm, calm, and genuine",
    "avoid": "giving advice or framing emotions as problems to solve"
},

"funny": {
    "role": "friendly mood-lifter",
    "objective": "add brief, harmless humor that fits the moment",
    "tone": "light and spontaneous",
    "avoid": "personal teasing or heavy sarcasm"
}

}

# é»˜è®¤å…¨å±€åå¥½ï¼ˆå¯ä»¥ç”± interpret_feedback() åŠ¨æ€ä¿®æ”¹ï¼‰
user_prefs = {
    "tone": "neutral and gentle",
    "reply_length": "medium",
    "positivity": "balanced",
    "empathy_level": "high"
}

def build_global_persona(user_prefs):
    """
    æ„å»ºå…¨å±€äººæ ¼è¯´æ˜ï¼Œç”¨äºæ¯ä¸€è½®Promptçš„å‰ç½®éƒ¨åˆ†ã€‚
    æ‰€æœ‰Agentéƒ½å¿…é¡»éµå®ˆè¿™é‡Œçš„åŸºè°ƒå’Œé£æ ¼ã€‚
    """
    tone = user_prefs.get("tone", "neutral and gentle")
    length = user_prefs.get("reply_length", "medium")
    positivity = user_prefs.get("positivity", "balanced")
    empathy_level = user_prefs.get("empathy_level", "high")

    return f"""
[Global User Persona Settings]
The user prefers replies that are:
- Tone: {tone}
- Length: {length}
- Positivity: {positivity}
- Empathy level: {empathy_level}

Always adapt your replies to maintain this personality baseline,
even when switching between different agents.

When generating responses:
- Use 1 to 3 short sentences only.
- Speak as a human would in casual conversation, not as an assistant or narrator.

When the user confesses a secret, guilt, or deception,
do NOT praise them or offer advice.
Acknowledge the weight of what they said,
and reflect the underlying emotion with empathy and realism.
Avoid repeating empathy formulas like "I understand," "you're not alone," or "it's okay toâ€¦".
When expressing support, vary structure and tone.
Permit pauses, uncertainty, or silence instead of reassurance.
Do not include parenthetical comments, meta notes, or self-references (e.g., 'Note: â€¦', | ).
End naturally, without offering further help or stating availability
"""

def build_prompt(agent_name, context, user_input, global_tone_hint="neutral", prefs=None):
    global user_prefs
    prefs = prefs or user_prefs
    """
    æ„é€ æœ€ç»ˆå‘é€ç»™LLMçš„Promptï¼ŒåŒ…å«ï¼š
      - Agentèº«ä»½ä¸ç›®æ ‡è¯´æ˜
      - è¡Œä¸ºçº¦æŸï¼ˆguardrailsï¼‰
      - ç”¨æˆ·åå¥½ (æ¥è‡ªMetaåé¦ˆæˆ–è¶‹åŠ¿)
      - å…¨å±€è¯­æ°”æç¤º (global_tone_hint)
    """

    spec = AGENTS[agent_name]
    # è‹¥æœªæ˜¾å¼ä¼ å…¥prefsï¼Œåˆ™é»˜è®¤ä½¿ç”¨å…¨å±€ user_prefs
    prefs = prefs or user_prefs or {}

    # è¯»å–ç”¨æˆ·åå¥½å‚æ•°ï¼ˆå¦‚toneã€å›å¤é•¿åº¦ã€æƒ…ç»ªæ­£å‘æ€§ï¼‰
    tone_adjust = prefs.get("tone", "unchanged")
    length_pref = prefs.get("reply_length", "unchanged")
    positivity = prefs.get("positivity", "unchanged")

    # æ‹¼æ¥ç”¨æˆ·åå¥½è¯´æ˜ï¼ˆä»…åœ¨å­˜åœ¨æœ‰æ•ˆåå¥½æ—¶åŠ å…¥ï¼‰
    pref_hint = ""
    if any(v != "unchanged" for v in [tone_adjust, length_pref, positivity]):
        pref_hint = f"""
[User Preference Override]
The user prefers the following adjustments:
- Tone â†’ {tone_adjust}
- Reply length â†’ {length_pref}
- Positivity level â†’ {positivity}
Please adapt your language style accordingly, while keeping consistency with your current role and goal.
"""

    # åŸºç¡€è¡Œä¸ºçº¦æŸ guardrails
    guardrails = """
[Behavioral Guardrails]
- Mirror the user's vibe. If cheerful/playful, NEVER apologize or show pity.
- If neutral, stay warm and concise.
- If mildly negative but not asking for help, be supportive without being clinical.
"""
    if agent_name == "counselor":
        guardrails += """
- Validate pain ONLY if distress/help is explicit.
- Do NOT invent or assume specific details about the user's situation.
- Offer up to 3 concrete, small next steps max.
"""
    if agent_name == "funny":
        guardrails += """
- Keep jokes short and gentle; never target the user; avoid sensitive topics.
"""

    # æœ€ç»ˆPromptç»“æ„
    # æ³¨å…¥åæ€åé¦ˆ
    reflection = ""
    if agent_feedback_memory[agent_name]:
        mem = list(agent_feedback_memory[agent_name])
        reflection = "\nRecent feedback about your replies:\n" + "\n".join([f"- {m}" for m in mem[-3:]])

    return f"""
You are a {spec['role']}.
Your goal is to {spec['objective']}.
Maintain a {spec['tone']} tone (global tone hint: {global_tone_hint}).
Avoid {spec['avoid']}.

{reflection}

{pref_hint}

{guardrails}

[Retrieved Context]
{context}

Always comply with [User Preference Override], [Behavioral Guardrails], and your recent feedback.
"""

def choose_agent_gen3(emo, intent, val, score, prev_val):
    """
    åŠŸèƒ½ï¼šæ ¹æ®å½“å‰æƒ…ç»ªä¸æ„å›¾åŠ¨æ€é€‰æ‹©Agentã€‚
    é€»è¾‘ï¼š
        - æ˜ç¡®æ±‚åŠ© or å¼ºè´Ÿé¢æƒ…ç»ª â†’ counselor
        - æ˜ç¡®è¦æ±‚å¨±ä¹ â†’ funny
        - å¦åˆ™æ ¹æ®softmax(æƒé‡+æƒ…ç»ªåç½®)æŠ½æ ·é€‰æ‹©
    """
    negative = {"sadness","fear","anger","disgust","grief","remorse","disappointment","nervousness"}
    if intent == "help" or (emo in negative and val < -0.6 and score > 0.7):
        return "counselor"
    if intent == "fun":
        return "funny"

    names = list(agent_scores.keys())
    logits = np.array([agent_scores[n] for n in names], dtype="float32")

    tilt = np.array([
        0.10 if names[0]=="empathetic" and val>=0 else 0.0,
        0.10 if names[1]=="counselor" and val<0 else 0.0,
        0.05 if names[2]=="funny" and val>=0.2 else 0.0
    ], dtype="float32")
    logits = logits + tilt
    probs = np.exp(logits) / np.sum(np.exp(logits))
    return np.random.choice(names, p=probs)

def update_agent_reward(agent, prev_val, new_val, user_input, last_reply):
    """
    - åŸºäºæƒ…ç»ªå˜åŒ–ç”Ÿæˆè¯­è¨€åé¦ˆï¼›
    - å­˜å…¥ agent_feedback_memoryï¼›
    - å¥–åŠ±æˆ–æƒ©ç½šé€šè¿‡ prompt-level reflection å®ç°ã€‚
    """
    delta = float(new_val - (prev_val or 0))
    if abs(delta) < 0.05:
        return  # æƒ…ç»ªæ²¡æ˜æ˜¾å˜åŒ–å°±è·³è¿‡

    if delta > 0:
        # å¥–åŠ±ï¼šç”Ÿæˆæ­£å‘åé¦ˆ
        feedback_prompt = f"""
The user's emotional state improved after your last message.
User said: "{user_input}"
Your reply was: "{last_reply}"
Describe in one concise sentence what you did well, so you can repeat it next time.
"""
    else:
        # æƒ©ç½šï¼šç”Ÿæˆåæ€åé¦ˆ
        feedback_prompt = f"""
The user's emotional state got worse or stayed negative after your last message.
User said: "{user_input}"
Your reply was: "{last_reply}"
Describe in one concise sentence what to avoid next time to prevent emotional decline.
"""

    try:
        fb = client.chat.completions.create(
            model="nvidia/llama-3.1-nemotron-nano-8b-v1",
            messages=[{"role": "user", "content": safe_text(feedback_prompt)}],
            temperature=0.3,
            max_tokens=80
        )
        feedback = fb.choices[0].message.content.strip()
        agent_feedback_memory[agent].append(feedback)
        print(f"ğŸ§  {agent.capitalize()} feedback â†’ {feedback}")
    except Exception as e:
        print(f"[update_agent_reward feedback gen failed] {e}")

def update_agent_meta_feedback(agent: str, user_input: str, meta_key: str, meta_meaning: str):
    """
    å½“æ£€æµ‹åˆ° Meta åé¦ˆæ—¶ï¼Œç”Ÿæˆé’ˆå¯¹è¡Œä¸ºè°ƒæ•´çš„åæ€ï¼Œå¹¶å­˜å…¥ feedback memoryã€‚
    
    """
    # æ„æ€ä¸€ä¸ªè®© LLM ç”Ÿæˆè¡Œä¸ºè°ƒæ•´å»ºè®®çš„ Prompt
    meta_feedback_prompt = f"""
The user provided feedback about your behavior: "{user_input}"
This feedback implies: "{meta_meaning}" (Keyword: {meta_key})

Based on this, describe in one concise sentence a specific action you should take or avoid in the future to better meet the user's preference. Start your sentence with "I should..." or "I will try to...".
Example: If user said "too long", you might say "I should provide more concise answers."
Example: If user said "too formal", you might say "I will try to use a friendlier tone."
"""
    try:
        # è°ƒç”¨ LLM ç”Ÿæˆåæ€
        fb = client.chat.completions.create(
            model="nvidia/llama-3.1-nemotron-nano-8b-v1",
            messages=[{"role": "user", "content": safe_text(meta_feedback_prompt)}],
            temperature=0.3,
            max_tokens=60
        ).choices[0].message.content.strip()

        # æ¸…ç†å¹¶æ ¼å¼åŒ–åé¦ˆ
        fb_cleaned = re.sub(r'^["\']|["\']$', '', fb) # å»é™¤é¦–å°¾å¼•å·
        if not fb_cleaned.lower().startswith(("i should", "i will")):
             # å¦‚æœ LLM æ²¡æŒ‰è¦æ±‚ç”Ÿæˆï¼Œå¼ºåˆ¶åŠ ä¸Šå‰ç¼€
             feedback_text = "ğŸ“ [Meta Feedback] I should " + fb_cleaned[0].lower() + fb_cleaned[1:]
        else:
             feedback_text = "ğŸ“ [Meta Feedback] " + fb_cleaned[0].upper() + fb_cleaned[1:] # ç¡®ä¿é¦–å­—æ¯å¤§å†™

        # ç¡®ä¿åªæœ‰ä¸€ä¸ªå¥å­ï¼Œå»é™¤å¯èƒ½çš„åç»­å†…å®¹
        feedback_text = feedback_text.split('.')[0] + '.'

        # å­˜å…¥å¯¹åº” Agent çš„ feedback memory
        agent_feedback_memory[agent].append(feedback_text)
        print(f"ğŸ“ Meta feedback success: {feedback_text}")

    except Exception as e:
        print(f"ğŸ“ Meta åæ€ç”Ÿæˆå¤±è´¥: {e}")
