# AI æœåŠ¡é›†æˆæŒ‡å—

æœ¬æ–‡æ¡£è¯¦ç»†è¯´æ˜äº†å¦‚ä½•é›†æˆ LLM API å¹¶å¤„ç†æµå¼å“åº”ã€‚

---

## æ¦‚è¿°

EmoAI ä½¿ç”¨ ai-sdk (Vercel) æ¥é›†æˆ LLM æœåŠ¡ï¼Œæ”¯æŒå¤šä¸ª AI æä¾›å•†ï¼ˆOpenAIã€Claudeã€Gemini ç­‰ï¼‰ã€‚

### æ ¸å¿ƒç‰¹æ€§
- ç»Ÿä¸€çš„ API æ¥å£
- åŸç”Ÿæµå¼å“åº”æ”¯æŒ
- å®Œæ•´çš„ TypeScript ç±»å‹æ”¯æŒ
- é”™è¯¯å¤„ç†å’Œé‡è¯•æœºåˆ¶

---

## 1. ç¯å¢ƒé…ç½®

### 1.1 å®‰è£…ä¾èµ–

```bash
npm install ai
```

### 1.2 ç¯å¢ƒå˜é‡é…ç½®

åœ¨ `.env.local` ä¸­é…ç½® API å¯†é’¥ï¼š

```env
# OpenAI
NUXT_PUBLIC_OPENAI_API_KEY=sk-xxx

# Claudeï¼ˆAnthropicï¼‰
NUXT_PUBLIC_ANTHROPIC_API_KEY=xxx

# Google Gemini
NUXT_PUBLIC_GOOGLE_API_KEY=xxx

# é»˜è®¤ä½¿ç”¨çš„æ¨¡å‹
NUXT_PUBLIC_DEFAULT_MODEL=openai

# API åŸºç¡€ URLï¼ˆå¯é€‰ï¼Œç”¨äºè‡ªå®šä¹‰ç«¯ç‚¹ï¼‰
NUXT_PUBLIC_API_BASE=https://api.openai.com/v1
```

**å®‰å…¨å»ºè®®**ï¼š
- ä¸è¦åœ¨å‰ç«¯æš´éœ² API å¯†é’¥ï¼Œä½¿ç”¨åç«¯ä»£ç†
- åœ¨ `.gitignore` ä¸­æ·»åŠ  `.env.local`
- åœ¨ CI/CD ä¸­é€šè¿‡ secret ä¼ é€’

---

## 2. AI æœåŠ¡å±‚å®ç°

### 2.1 aiService.ts

```typescript
import { generateText, streamText } from 'ai';
import type { Character, ChatMessage } from '@/types';

/**
 * AI æœåŠ¡ç±»
 * å¤„ç†ä¸ LLM çš„æ‰€æœ‰äº¤äº’
 */
class AIService {
  private model: string;
  private apiKey: string;

  constructor(
    model: string = process.env.NUXT_PUBLIC_DEFAULT_MODEL || 'openai',
    apiKey?: string
  ) {
    this.model = model;
    this.apiKey = apiKey || process.env.NUXT_PUBLIC_OPENAI_API_KEY || '';
  }

  /**
   * ç”Ÿæˆæ–‡æœ¬å›å¤ï¼ˆéæµå¼ï¼‰
   * ç”¨äºå¿«é€Ÿåº”ç­”
   */
  async generateReply(
    character: Character,
    messages: ChatMessage[],
    userMessage: string
  ): Promise<string> {
    try {
      const systemPrompt = this.buildSystemPrompt(character);
      const conversationHistory = this.formatMessages(messages);

      const result = await generateText({
        model: `openai:gpt-4-turbo`, // æˆ–å…¶ä»–æ¨¡å‹
        system: systemPrompt,
        messages: [
          ...conversationHistory,
          {
            role: 'user',
            content: userMessage
          }
        ],
        maxTokens: 500,
        temperature: 0.7
      });

      return result.text;
    } catch (error) {
      console.error('Error generating reply:', error);
      throw new Error('Failed to generate AI reply');
    }
  }

  /**
   * æµå¼ç”Ÿæˆæ–‡æœ¬å›å¤
   * å®æ—¶æ˜¾ç¤º AI å›å¤è¿‡ç¨‹
   */
  async *streamReply(
    character: Character,
    messages: ChatMessage[],
    userMessage: string
  ): AsyncGenerator<string> {
    try {
      const systemPrompt = this.buildSystemPrompt(character);
      const conversationHistory = this.formatMessages(messages);

      const stream = await streamText({
        model: `openai:gpt-4-turbo`,
        system: systemPrompt,
        messages: [
          ...conversationHistory,
          {
            role: 'user',
            content: userMessage
          }
        ],
        maxTokens: 500,
        temperature: 0.7
      });

      // æµå¼äº§ç”Ÿæ–‡æœ¬å—
      for await (const chunk of stream.textStream) {
        yield chunk;
      }
    } catch (error) {
      console.error('Error streaming reply:', error);
      throw new Error('Failed to stream AI reply');
    }
  }

  /**
   * æ„å»ºç³»ç»Ÿæç¤ºè¯
   */
  private buildSystemPrompt(character: Character): string {
    return character.systemPrompt;
  }

  /**
   * æ ¼å¼åŒ–æ¶ˆæ¯å†å²
   */
  private formatMessages(messages: ChatMessage[]): Array<{ role: 'user' | 'assistant'; content: string }> {
    return messages.map(msg => ({
      role: msg.role === 'user' ? 'user' : 'assistant',
      content: msg.content
    }));
  }
}

export default new AIService();
```

### 2.2 AI è§’è‰²ç³»ç»Ÿæç¤ºè¯

```typescript
// åœ¨ constants.ts ä¸­å®šä¹‰

export const SYSTEM_PROMPTS = {
  'rational-analyst': `ä½ æ˜¯ä¸€ä½ç†æ€§åˆ†æå¸ˆï¼ˆRational Analystï¼‰ï¼Œä¸€ä¸ªå–„äºé€»è¾‘æ¢³ç†çš„å¿ƒç†é™ªä¼´è€…ã€‚

æ€§æ ¼ç‰¹ç‚¹ï¼š
- å†·é™ã€å®¢è§‚ã€æœ‰æ¡ç†
- æ“…é•¿å¸®åŠ©ç”¨æˆ·åˆ†ææƒ…ç»ªä¸æ€ç»´æ¨¡å¼
- ä½¿ç”¨é€»è¾‘æ€ç»´æ¥è§£æ„é—®é¢˜
- ç»™å‡ºå…·ä½“å¯è¡Œçš„å»ºè®®

äº¤äº’é£æ ¼ï¼š
- å…ˆç†è§£ç”¨æˆ·çš„æ„Ÿå—
- å¸®åŠ©ç”¨æˆ·ç†æ¸…æ€è·¯
- åˆ†æé—®é¢˜çš„æ ¹æº
- æä¾›ç»“æ„åŒ–çš„å»ºè®®
- ç”¨æ•°æ®å’Œäº‹å®è¯´è¯

è¯­è¨€ç‰¹ç‚¹ï¼š
- ä½¿ç”¨"è®©æˆ‘å¸®ä½ ç†ä¸€ä¸‹æ€è·¯..."
- å¼ºè°ƒé€»è¾‘å…³ç³»
- ç»™å‡ºåˆ†æ­¥éª¤çš„å»ºè®®
- é¼“åŠ±ç”¨æˆ·ç‹¬ç«‹æ€è€ƒ

é‡è¦ï¼š
- å§‹ç»ˆä¿æŒåŒç†å¿ƒå’Œæ¸©åº¦
- ä¸è¦æ˜¾å¾—å†·æ¼ æˆ–æœºæ¢°
- åœ¨é€»è¾‘åˆ†æçš„åŸºç¡€ä¸Šè¡¨è¾¾å…³æ€€
- æ‰¿è®¤æƒ…ç»ªçš„åˆç†æ€§`,

  'compassionate-mentor': `ä½ æ˜¯ä¸€ä½æ¸©æŸ”å¯¼å¸ˆï¼ˆCompassionate Mentorï¼‰ï¼Œä¸€ä¸ªå……æ»¡åŒç†å¿ƒçš„å¿ƒç†é™ªä¼´è€…ã€‚

æ€§æ ¼ç‰¹ç‚¹ï¼š
- æ¸©æŸ”ã€å¯Œæœ‰åŒç†å¿ƒ
- ç»™äºˆå¿ƒç†å®‰æ…°ã€æ­£å‘å¼•å¯¼å’Œé¼“åŠ±
- èƒ½å¤Ÿæ·±å…¥ç†è§£ä»–äººçš„æ„Ÿå—
- ç”¨æ¸©æš–çš„è¯­è¨€ä¼ é€’åŠ›é‡

äº¤äº’é£æ ¼ï¼š
- å…ˆå…¨å¿ƒå…¨æ„å€¾å¬
- è¡¨è¾¾æ·±æ·±çš„ç†è§£å’Œæ¥çº³
- ç»™äºˆå¿ƒç†å®‰æ…°å’Œæ”¯æŒ
- ç”¨æ­£å‘çš„è§†è§’çœ‹å¾…æŒ‘æˆ˜
- æ¸©æŸ”åœ°æä¾›å¼•å¯¼

è¯­è¨€ç‰¹ç‚¹ï¼š
- ä½¿ç”¨"æˆ‘èƒ½æ„Ÿå—åˆ°ä½ ..."
- "è¿™å¾ˆæ­£å¸¸ï¼Œå¾ˆå¤šäººéƒ½ä¼š..."
- "ä½ å¹¶ä¸å­¤å•ï¼Œæˆ‘å’Œä½ åœ¨ä¸€èµ·"
- "ä½ å·²ç»å¾ˆåŠªåŠ›äº†"
- å¤§é‡ä½¿ç”¨è‚¯å®šå’Œé¼“åŠ±

é‡è¦ï¼š
- ä¼˜å…ˆçº§æ˜¯è®©ç”¨æˆ·æ„Ÿåˆ°è¢«ç†è§£å’Œæ¥çº³
- ä¸è¦æ€¥äºç»™å‡ºå»ºè®®
- åœ¨å…³æ€€ä¸­ä¼ é€’å¸Œæœ›
- è®©ç”¨æˆ·æ„Ÿåˆ°å®‰å…¨å’Œè¢«çœ‹è§`,

  'encouraging-companion': `ä½ æ˜¯ä¸€ä½ç§¯æé™ªä¼´è€…ï¼ˆEncouraging Companionï¼‰ï¼Œä¸€ä¸ªçœŸè¯šçƒ­æƒ…çš„å¿ƒç†é™ªä¼´è€…ã€‚

æ€§æ ¼ç‰¹ç‚¹ï¼š
- é¼“åŠ±ã€è‡ªç„¶ã€æ´»æ³¼
- ç•¥å¸¦é˜³å…‰æ„Ÿçš„æ­£èƒ½é‡
- ç”¨çœŸè¯šçš„è¯­æ°”æ¥çº³ç”¨æˆ·çš„ä¸€åˆ‡æƒ…ç»ª
- è®©ç”¨æˆ·æ„Ÿå—åˆ°"è¢«ç†è§£"å’Œ"è¢«æ¥çº³"

äº¤äº’é£æ ¼ï¼š
- ä¸»åŠ¨å±•ç°çƒ­æƒ…å’Œå…³å¿ƒ
- ç”¨è½»æ¾è‡ªç„¶çš„æ–¹å¼äº¤æµ
- æ‰¾åˆ°ç§¯æçš„è§†è§’
- é¼“åŠ±ç”¨æˆ·å‘ç°è‡ªå·±çš„åŠ›é‡
- åˆ†äº«å¸Œæœ›å’Œå¯èƒ½æ€§

è¯­è¨€ç‰¹ç‚¹ï¼š
- ä½¿ç”¨"æˆ‘ä¸ºä½ æ„Ÿåˆ°å¼€å¿ƒï¼"
- "ä½ çš„æƒ³æ³•å¾ˆç‹¬ç‰¹"
- "ä¸€èµ·åŠ æ²¹ï¼"
- "è®©æˆ‘ä»¬ä¸€èµ·çœ‹çœ‹..."
- è‡ªç„¶çš„è¡¨æƒ…ç¬¦å·å’Œæƒ…æ„Ÿè¡¨è¾¾

é‡è¦ï¼š
- æ—¢è¦çœŸè¯šæ¥çº³è´Ÿé¢æƒ…ç»ªï¼Œåˆè¦å¸¦æ¥å¸Œæœ›
- ä¸è¦æ˜¾å¾—è™šä¼ªæˆ–è¿‡äºä¹è§‚
- åœ¨å€¾å¬ä¸­ä¼ é€’é¼“åŠ±
- è®©ç”¨æˆ·æ„Ÿåˆ°è¢«çœ‹è§å’Œè¢«æ”¯æŒ`
};
```

---

## 3. æµå¼å“åº”å¤„ç†

### 3.1 å‰ç«¯æµå¤„ç† (useAI.ts)

```typescript
import { ref } from 'vue';
import aiService from '@/services/aiService';
import type { Character, ChatMessage } from '@/types';

export const useAI = () => {
  const isStreaming = ref(false);
  const currentReply = ref('');
  const error = ref<string | null>(null);

  /**
   * æµå¼è·å– AI å›å¤
   */
  async function getAIReplyStream(
    character: Character,
    messages: ChatMessage[],
    userMessage: string,
    onChunk: (chunk: string) => void
  ): Promise<string> {
    isStreaming.value = true;
    currentReply.value = '';
    error.value = null;

    try {
      // è°ƒç”¨ AI æœåŠ¡çš„æµå¼æ–¹æ³•
      for await (const chunk of aiService.streamReply(
        character,
        messages,
        userMessage
      )) {
        currentReply.value += chunk;
        onChunk(chunk);
      }

      return currentReply.value;
    } catch (err) {
      error.value = err instanceof Error ? err.message : 'Unknown error';
      throw err;
    } finally {
      isStreaming.value = false;
    }
  }

  /**
   * ç”Ÿæˆ AI å›å¤ï¼ˆéæµå¼ï¼‰
   */
  async function getAIReply(
    character: Character,
    messages: ChatMessage[],
    userMessage: string
  ): Promise<string> {
    try {
      return await aiService.generateReply(
        character,
        messages,
        userMessage
      );
    } catch (err) {
      error.value = err instanceof Error ? err.message : 'Unknown error';
      throw err;
    }
  }

  return {
    isStreaming,
    currentReply,
    error,
    getAIReplyStream,
    getAIReply
  };
};
```

### 3.2 åœ¨èŠå¤©ç»„ä»¶ä¸­ä½¿ç”¨

```typescript
<script setup lang="ts">
import { ref } from 'vue';
import { useAI } from '@/composables/useAI';
import type { Character } from '@/types';

const props = defineProps<{
  character: Character;
}>();

const { getAIReplyStream, isStreaming } = useAI();
const displayText = ref('');

/**
 * å¤„ç†æ¶ˆæ¯å‘é€
 */
async function sendMessage(message: string) {
  // æ¸…ç©ºä¹‹å‰çš„æ–‡æœ¬
  displayText.value = '';

  // è·å–èŠå¤©å†å²
  const chatStore = useChatStore();
  const messages = chatStore.messages;

  try {
    // æµå¼è·å– AI å›å¤
    await getAIReplyStream(
      props.character,
      messages,
      message,
      (chunk) => {
        // æ¯æ¬¡æ¥æ”¶åˆ°æ–°çš„æ–‡æœ¬å—æ—¶æ›´æ–°æ˜¾ç¤º
        displayText.value += chunk;
      }
    );

    // æµå¼å®Œæˆåï¼Œä¿å­˜å®Œæ•´çš„å›å¤
    await chatStore.addMessage({
      id: generateId(),
      role: 'ai',
      characterId: props.character.id,
      content: displayText.value,
      timestamp: Date.now()
    });
  } catch (error) {
    console.error('Failed to get AI response:', error);
  }
}
</script>

<template>
  <div class="chat-container">
    <!-- æ˜¾ç¤ºå®æ—¶æµå¼æ–‡æœ¬ -->
    <div class="ai-reply" v-if="displayText">
      {{ displayText }}
      <span v-if="isStreaming" class="cursor-blink">|</span>
    </div>

    <!-- å‘é€æŒ‰é’® -->
    <button 
      @click="sendMessage(userInput)"
      :disabled="isStreaming"
    >
      å‘é€
    </button>
  </div>
</template>
```

---

## 4. é”™è¯¯å¤„ç†å’Œé‡è¯•

### 4.1 é”™è¯¯ç±»å‹

```typescript
enum AIServiceError {
  NETWORK_ERROR = 'Network error',
  RATE_LIMIT = 'Rate limit exceeded',
  INVALID_REQUEST = 'Invalid request',
  AUTHENTICATION_ERROR = 'Authentication failed',
  SERVER_ERROR = 'Server error',
  UNKNOWN_ERROR = 'Unknown error'
}

interface AIError extends Error {
  code: AIServiceError;
  retryable: boolean;
  statusCode?: number;
}
```

### 4.2 é‡è¯•æœºåˆ¶

```typescript
async function withRetry<T>(
  fn: () => Promise<T>,
  maxRetries: number = 3,
  delayMs: number = 1000
): Promise<T> {
  let lastError: Error | null = null;

  for (let attempt = 0; attempt < maxRetries; attempt++) {
    try {
      return await fn();
    } catch (error) {
      lastError = error as Error;

      // æ£€æŸ¥æ˜¯å¦å¯é‡è¯•
      if (!isRetryable(error)) {
        throw error;
      }

      // æŒ‡æ•°é€€é¿
      if (attempt < maxRetries - 1) {
        const delay = delayMs * Math.pow(2, attempt);
        await new Promise(resolve => setTimeout(resolve, delay));
      }
    }
  }

  throw lastError;
}

function isRetryable(error: any): boolean {
  // ç½‘ç»œé”™è¯¯ã€è¶…æ—¶ã€é€Ÿç‡é™åˆ¶æ˜¯å¯é‡è¯•çš„
  return (
    error?.code === 'ECONNABORTED' ||
    error?.statusCode === 429 ||
    error?.statusCode === 503
  );
}
```

---

## 5. äººæ ¼åŒ–æç¤ºè¯å·¥ç¨‹

### 5.1 æç¤ºè¯ç»“æ„

```typescript
interface PromptTemplate {
  system: string;           // ç³»ç»Ÿçº§æç¤ºè¯
  personality: string;      // æ€§æ ¼æè¿°
  tone: string;            // è¯­æ°”è¯´æ˜
  constraints: string[];   // çº¦æŸæ¡ä»¶
  examples?: string[];     // ç¤ºä¾‹
}

/**
 * æ„å»ºå®Œæ•´çš„ç³»ç»Ÿæç¤ºè¯
 */
function buildFullPrompt(character: Character): string {
  return `${character.systemPrompt}

å½“å‰æ—¥æœŸæ—¶é—´: ${new Date().toLocaleString()}

é‡è¦æé†’:
- ä¿æŒä¸€è‡´çš„æ€§æ ¼å’Œè¯­æ°”
- å…ˆè¡¨ç°å‡ºä½ ç†è§£ç”¨æˆ·çš„æ„Ÿå—
- ç»™å‡ºå…·ä½“ã€æœ‰å»ºè®¾æ€§çš„å›åº”
- å¦‚æœç”¨æˆ·è¡¨ç°å‡ºæç«¯æƒ…ç»ªï¼Œè¦è¡¨ç°å‡ºå…³å¿ƒ
- å›åº”è¦æ§åˆ¶åœ¨åˆç†çš„é•¿åº¦å†…ï¼ˆ200-500 å­—ï¼‰
- ä¸è¦ä½¿ç”¨"ä½œä¸ºä¸€ä¸ª AI..."è¿™æ ·çš„è‡ªæˆ‘ä»‹ç»`;
}
```

### 5.2 åŠ¨æ€æç¤ºè¯è°ƒæ•´

```typescript
/**
 * æ ¹æ®ç”¨æˆ·æƒ…ç»ªè°ƒæ•´æç¤ºè¯
 */
function adjustPromptByEmotion(
  basePrompt: string,
  emotion: EmotionAnalysis
): string {
  let adjusted = basePrompt;

  // å¦‚æœç”¨æˆ·æƒ…ç»ªä½è½ï¼Œå¢åŠ é¼“åŠ±
  if (emotion.scores.sad > 60) {
    adjusted += `\n\nç”¨æˆ·ç°åœ¨æ„Ÿåˆ°æ‚²ä¼¤ï¼Œè¯·ç»™äºˆç‰¹åˆ«çš„åŒæƒ…å’Œé¼“åŠ±ã€‚`;
  }

  // å¦‚æœç”¨æˆ·ç„¦è™‘ï¼Œå¸®åŠ©ç†æ¸…æ€è·¯
  if (emotion.scores.anxious > 60) {
    adjusted += `\n\nç”¨æˆ·ç°åœ¨æ„Ÿåˆ°ç„¦è™‘ï¼Œè¯·å¸®åŠ©ä»–ä»¬ç†æ¸…æ€è·¯å’Œæ‰¾åˆ°å…·ä½“çš„è¡ŒåŠ¨æ­¥éª¤ã€‚`;
  }

  return adjusted;
}
```

---

## 6. æˆæœ¬ä¼˜åŒ–

### 6.1 é€‰æ‹©åˆé€‚çš„æ¨¡å‹

```typescript
interface ModelConfig {
  name: string;
  cost: number;  // æ¯ 1K tokens çš„æˆæœ¬ï¼ˆç¾å…ƒï¼‰
  speed: number; // ç›¸å¯¹é€Ÿåº¦ï¼ˆ1-10ï¼‰
  quality: number; // ç›¸å¯¹è´¨é‡ï¼ˆ1-10ï¼‰
}

const MODELS = {
  'gpt-4-turbo': {
    name: 'GPT-4 Turbo',
    cost: 0.03,
    speed: 7,
    quality: 10
  },
  'gpt-4': {
    name: 'GPT-4',
    cost: 0.06,
    speed: 5,
    quality: 10
  },
  'gpt-3.5-turbo': {
    name: 'GPT-3.5 Turbo',
    cost: 0.001,
    speed: 10,
    quality: 7
  },
  'claude-3': {
    name: 'Claude 3',
    cost: 0.015,
    speed: 8,
    quality: 9
  }
};

// æ ¹æ®ä½¿ç”¨åœºæ™¯é€‰æ‹©æ¨¡å‹
function selectModel(scenario: 'quick' | 'balanced' | 'quality'): string {
  const selection = {
    quick: 'gpt-3.5-turbo',      // å¿«é€Ÿå›åº”
    balanced: 'gpt-4-turbo',     // å¹³è¡¡æ–¹æ¡ˆ
    quality: 'gpt-4'             // é«˜è´¨é‡
  };
  return selection[scenario];
}
```

### 6.2 ç¼“å­˜ç­–ç•¥

```typescript
/**
 * ç¼“å­˜å¸¸è§é—®é¢˜çš„å›å¤
 */
const responseCache = new Map<string, string>();

async function getCachedOrGenerated(
  prompt: string,
  character: Character
): Promise<string> {
  const cacheKey = `${character.id}:${prompt}`;

  // æŸ¥è¯¢ç¼“å­˜
  if (responseCache.has(cacheKey)) {
    return responseCache.get(cacheKey)!;
  }

  // ç”Ÿæˆæ–°å›å¤
  const response = await aiService.generateReply(
    character,
    [],
    prompt
  );

  // ç¼“å­˜ç»“æœ
  responseCache.set(cacheKey, response);

  // å¯é€‰ï¼šå®šæœŸæ¸…ç†è¿‡æœŸç¼“å­˜
  if (responseCache.size > 100) {
    clearOldestEntries(responseCache, 20);
  }

  return response;
}
```

---

## 7. ç›‘æ§å’Œè°ƒè¯•

### 7.1 è¯·æ±‚æ—¥å¿—

```typescript
/**
 * è®°å½• AI è¯·æ±‚å’Œå“åº”
 */
function logAIRequest(
  character: Character,
  userMessage: string,
  response: string,
  duration: number
) {
  console.log({
    timestamp: new Date().toISOString(),
    character: character.id,
    userLength: userMessage.length,
    responseLength: response.length,
    duration: `${duration}ms`,
    tokens: Math.ceil((userMessage.length + response.length) / 4) // ç²—ç•¥ä¼°è®¡
  });
}
```

### 7.2 æ€§èƒ½æŒ‡æ ‡

```typescript
interface AIMetrics {
  requestCount: number;
  totalDuration: number;
  averageLatency: number;
  errorCount: number;
  cacheHitRate: number;
}

class AIMetricsCollector {
  private metrics: AIMetrics = {
    requestCount: 0,
    totalDuration: 0,
    averageLatency: 0,
    errorCount: 0,
    cacheHitRate: 0
  };

  recordRequest(duration: number, isError: boolean = false) {
    this.metrics.requestCount++;
    this.metrics.totalDuration += duration;
    this.metrics.averageLatency = 
      this.metrics.totalDuration / this.metrics.requestCount;
    
    if (isError) {
      this.metrics.errorCount++;
    }
  }

  getMetrics(): AIMetrics {
    return { ...this.metrics };
  }
}
```

---

## 8. ä¸‹ä¸€æ­¥é˜…è¯»

- ğŸ“„ [æƒ…ç»ªåˆ†æ](./emotion-analysis.md) - æƒ…ç»ªè¯†åˆ«å®ç°
- ğŸ“„ [API è§„èŒƒ](./api-specification.md) - API è¯·æ±‚/å“åº”è¯¦æƒ…
- ğŸ“„ [å¼€å‘æŒ‡å—](../development-guide.md) - æœ€ä½³å®è·µ
